Participants: Benoit, Deepika

1. Finalize case studies (number of methods, methods invoked (including private ones), ...): 
  - PDFBox
  - ttorrent
  - jitsi (feasible?)
2. Find depth in call stack of invoked methods (profile)
  - for methods like `isSomeCondition()`, test cases are extremely important
  - such methods likely to not be modified very frequently
  - large number of invocations
3. Plot number-of-invocations, number-of-unique-invocations Vs. methods of interest
4. Plot depth, mean-depth Vs. number-of-invocations
5. Draft RQs in overleaf:
  - RQ1: How many test cases can we generate => feasibility, statistics about generated tests
  - RQ2: How do these generated tests differ from developer test cases => input space, diversity, coverage
  - RQ3: How do the generated test cases complement the developer tests (test suite) => mutation
6. For test cases that pass only after slight modifications, we discuss them in the paper in the discussions/threats section
  - we instrument, we obtain objects, we generate compilable test cases, we create objects too
  - if production objects look identical to object returned from invocation, but assertion still fails, probably something missing in the pipeline
  - we still manage to make them pass after x and y modifications
7. Think of name for the whole pipeline (application => [] => test cases)
  - trace, test, oracle, production, regression, abstraction, generation, ...
8. Venue: ~~DSN~~ IEEE Transactions on Reliability, December 1
9. Myra Cohen (editor): see paper ["An orchestrated survey of methodologies for automated software test case generation"](https://romisatriawahono.net/lecture/rm/survey/software%20engineering/Software%20Testing/Anand%20-%20Automated%20Software%20Test%20Case%20generation%20-%202013.pdf)

