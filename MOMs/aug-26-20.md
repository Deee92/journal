Participants: Benoit, Deepika

1. Submission ideas (non-exhaustive): [<Programming> 2021](https://2021.programming-conference.org/);

2. Expanding the set of candidate methods
- Identify the immediate callers of private methods => pure but with invocations of these methods
- Static call graph to find set of public callers
- Dynamic call graph to find the closest public caller
- Are these methods still pure, except for this invocation
- If so, we expand the set of candidates
- Problem: if the application is multithreaded, reading objects can also make the method undeterministic; we don't claim that our set of methods is "pure".

3. Reasoning about failing test cases
- Deep equality comparison of expected and actual objects to see why/how they are not equal
  - If they are equal, potential problem in serialization-deserialization loop?
  - Examine the method for which the cases are failing: is it undeterministic, otherwise "weird"?
- String length problem (read from test resource)

4. Finding more case studies
- jitsi - identify candidate methods for version that is running
  - Generate plugin to see how many times, which methods are called
  - Generate plugin for methods to serialize objects

5. How different (in terms of values, complexity) are objects written as test inputs by developers from values we actually observe (and serialize) during runtime?

6. Metrics: coverage, mutation analysis, of methods in original TS Vs. our generated TS

7. Writing and RQs: Kinds and complexities/depth of objects we collect (+ visualization potentially), test cases we generate that pass and fail, how they amplify the existing TS

8. TA duties, courses

